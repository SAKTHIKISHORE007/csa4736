{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb5FrteIRK33L8O7KYeT0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAKTHIKISHORE007/csa4736/blob/main/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name='t5-small'):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def preprocess_data(self, texts, summaries, max_input_length=512, max_target_length=150):\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            max_length=max_input_length,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        targets = self.tokenizer(\n",
        "            summaries,\n",
        "            max_length=max_target_length,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "    def train(self, train_texts, train_summaries, val_texts=None, val_summaries=None, epochs=3):\n",
        "        # Prepare training data\n",
        "        train_inputs, train_targets = self.preprocess_data(train_texts, train_summaries)\n",
        "\n",
        "        # Optional validation split\n",
        "        if val_texts is None:\n",
        "            train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
        "                train_texts, train_summaries, test_size=0.2\n",
        "            )\n",
        "            train_inputs, train_targets = self.preprocess_data(train_texts, train_summaries)\n",
        "            val_inputs, val_targets = self.preprocess_data(val_texts, val_summaries)\n",
        "\n",
        "        # Training loop\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(\n",
        "                input_ids=train_inputs['input_ids'].to(self.device),\n",
        "                attention_mask=train_inputs['attention_mask'].to(self.device),\n",
        "                labels=train_targets['input_ids'].to(self.device)\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def generate_summary(self, text, max_length=150, num_return_sequences=1):\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            max_length=512,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        summary_ids = self.model.generate(\n",
        "            inputs.input_ids,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            max_length=max_length,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summaries = [\n",
        "            self.tokenizer.decode(g, skip_special_tokens=True)\n",
        "            for g in summary_ids\n",
        "        ]\n",
        "\n",
        "        return summaries\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "    # Sample long text\n",
        "    long_text = \"\"\"\n",
        "    Artificial Intelligence is transforming multiple industries\n",
        "    by providing advanced computational capabilities. Machine learning\n",
        "    algorithms can now process vast amounts of data, recognize patterns,\n",
        "    and make intelligent decisions across various domains including\n",
        "    healthcare, finance, and technology sectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer.generate_summary(long_text)\n",
        "    print(\"Original Text:\", long_text)\n",
        "    print(\"Generated Summary:\", summary[0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UmK0wOv0k2qz",
        "outputId": "e90c2993-376d-4c73-8d57-979c0f4b3809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: \n",
            "    Artificial Intelligence is transforming multiple industries \n",
            "    by providing advanced computational capabilities. Machine learning \n",
            "    algorithms can now process vast amounts of data, recognize patterns, \n",
            "    and make intelligent decisions across various domains including \n",
            "    healthcare, finance, and technology sectors.\n",
            "    \n",
            "Generated Summary: Intelligence is transforming multiple industries by providing advanced computational capabilities. Machine learning algorithms can process vast amounts of data, recognize patterns, and make intelligent decisions across various domains including healthcare, finance, and technology sectors.\n"
          ]
        }
      ]
    }
  ]
}